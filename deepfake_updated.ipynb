{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c856802e-9ca1-47c8-b557-10684600b03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, time\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a62b7729-f354-40d0-b731-53f63fba29bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_per_video = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d022704c-42b5-459a-9130-e9740d7fa01c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.1; however, version 24.2 is available.\n",
      "You should consider upgrading via the 'c:\\users\\human\\env\\scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install ./xception/pytorchcv-0.0.55-py2.py3-none-any.whl --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3e7feecd-e327-4ffa-80c9-dd505e182065",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2161"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dir = \"C:/Users/human/Downloads/sih/test/fake\"\n",
    "\n",
    "test_videos = sorted([x for x in os.listdir(test_dir) if x[-4:] == \".mp4\"])\n",
    "len(test_videos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4f60235d-ae86-4ec3-b297-79fd7db1a280",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3e016b69-5545-4915-8a41-30bf6053602a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fa64dbeb-f11d-4cd3-a3a3-c78c893fc8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Normalize\n",
    "\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "normalize_transform = Normalize(mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e894755f-a3af-4238-96e1-76e4245d05e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def isotropically_resize_image(img, size, resample=cv2.INTER_AREA):\n",
    "    h, w = img.shape[:2]\n",
    "    if w > h:\n",
    "        h = h * size // w\n",
    "        w = size\n",
    "    else:\n",
    "        w = w * size // h\n",
    "        h = size\n",
    "\n",
    "    resized = cv2.resize(img, (w, h), interpolation=resample)\n",
    "    return resized\n",
    "\n",
    "\n",
    "def make_square_image(img):\n",
    "    h, w = img.shape[:2]\n",
    "    size = max(h, w)\n",
    "    t = 0\n",
    "    b = size - h\n",
    "    l = 0\n",
    "    r = size - w\n",
    "    return cv2.copyMakeBorder(img, t, b, l, r, cv2.BORDER_CONSTANT, value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "eb61d3ee-9f15-49b9-8bfb-1fc3239bd13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorchcv.model_provider import get_model\n",
    "model = get_model(\"xception\", pretrained=True)\n",
    "model = nn.Sequential(*list(model.children())[:-1]) # Remove original output layer\n",
    "\n",
    "class Pooling(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Pooling, self).__init__()\n",
    "    \n",
    "    self.p1 = nn.AdaptiveAvgPool2d((1,1))\n",
    "    self.p2 = nn.AdaptiveMaxPool2d((1,1))\n",
    "\n",
    "  def forward(self, x):\n",
    "    x1 = self.p1(x)\n",
    "    x2 = self.p2(x)\n",
    "    return (x1+x2) * 0.5\n",
    "\n",
    "model[0].final_block.pool = nn.Sequential(nn.AdaptiveAvgPool2d((1,1)))\n",
    "\n",
    "class Head(torch.nn.Module):\n",
    "  def __init__(self, in_f, out_f):\n",
    "    super(Head, self).__init__()\n",
    "    \n",
    "    self.f = nn.Flatten()\n",
    "    self.l = nn.Linear(in_f, 512)\n",
    "    self.d = nn.Dropout(0.5)\n",
    "    self.o = nn.Linear(512, out_f)\n",
    "    self.b1 = nn.BatchNorm1d(in_f)\n",
    "    self.b2 = nn.BatchNorm1d(512)\n",
    "    self.r = nn.ReLU()\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.f(x)\n",
    "    x = self.b1(x)\n",
    "    x = self.d(x)\n",
    "\n",
    "    x = self.l(x)\n",
    "    x = self.r(x)\n",
    "    x = self.b2(x)\n",
    "    x = self.d(x)\n",
    "\n",
    "    out = self.o(x)\n",
    "    return out\n",
    "\n",
    "class FCN(torch.nn.Module):\n",
    "  def __init__(self, base, in_f):\n",
    "    super(FCN, self).__init__()\n",
    "    self.base = base\n",
    "    self.h1 = Head(in_f, 1)\n",
    "  \n",
    "  def forward(self, x):\n",
    "    x = self.base(x)\n",
    "    return self.h1(x)\n",
    "\n",
    "net = []\n",
    "model = FCN(model, 2048)\n",
    "# model = model.cuda()\n",
    "model.load_state_dict(torch.load('./xception/model_v0.pth', map_location=torch.device('cpu')))\n",
    "net.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ed7f2865-4653-4c1c-b3dc-6dee557c8836",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the pre-trained Haar Cascade model for face detection\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "def extract_faces_from_video(video_path, output_folder):\n",
    "    # Create the output folder if it does not exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Unable to open video file {video_path}\")\n",
    "        return\n",
    "    \n",
    "    frame_number = 0\n",
    "    face_number = 0\n",
    "    flag = 0\n",
    "    while True:\n",
    "        # Read a frame from the video\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Detect faces in the frame\n",
    "        faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "        \n",
    "        # Iterate over detected faces and save them\n",
    "        for (x, y, w, h) in faces:\n",
    "            face = frame[y:y+h, x:x+w]\n",
    "            face_filename = os.path.join(output_folder, f\"frame{frame_number}_face{face_number}.jpg\")\n",
    "            cv2.imwrite(face_filename, face)\n",
    "            face_number += 1\n",
    "            if(face_number >= 100): \n",
    "                flag = 1 \n",
    "                break\n",
    "        if (flag):\n",
    "            break;\n",
    "        frame_number += 1\n",
    "    \n",
    "    # Release the video capture object\n",
    "    cap.release()\n",
    "    print(f\"Extraction complete. {face_number} faces extracted.\")\n",
    "\n",
    "\n",
    "def predict_on_video(video_path, batch_size):\n",
    "    try:\n",
    "        # Directory to temporarily save extracted faces\n",
    "        temp_faces_directory = \"./temp_faces\"\n",
    "        os.makedirs(temp_faces_directory, exist_ok=True)\n",
    "\n",
    "        # Extract faces from the video using the custom function\n",
    "        extract_faces_from_video(video_path, temp_faces_directory)\n",
    "\n",
    "        # List all face images in the directory\n",
    "        face_files = os.listdir(temp_faces_directory)\n",
    "\n",
    "        if not face_files:\n",
    "            print(\"No faces found in video:\", video_path)\n",
    "            return 0.5\n",
    "\n",
    "        # Initialize array for batch processing\n",
    "        x = np.zeros((batch_size, input_size, input_size, 3), dtype=np.uint8)\n",
    "        n = 0\n",
    "\n",
    "        for face_file in face_files:\n",
    "            face_path = os.path.join(temp_faces_directory, face_file)\n",
    "            face = cv2.imread(face_path)\n",
    "\n",
    "            # Preprocess face as needed for your model\n",
    "            resized_face = isotropically_resize_image(face, input_size)\n",
    "            resized_face = make_square_image(resized_face)\n",
    "\n",
    "            if n < batch_size:\n",
    "                x[n] = resized_face\n",
    "                n += 1\n",
    "            else:\n",
    "                print(\"WARNING: have %d faces but batch size is %d\" % (n, batch_size))\n",
    "\n",
    "        if n > 0:\n",
    "            x = torch.tensor(x).float()\n",
    "            x = x.permute((0, 3, 1, 2))\n",
    "\n",
    "            for i in range(len(x)):\n",
    "                x[i] = normalize_transform(x[i] / 255.)\n",
    "\n",
    "            # Make predictions with the model\n",
    "            with torch.no_grad():\n",
    "                y_pred = model(x)\n",
    "                y_pred = torch.sigmoid(y_pred.squeeze())\n",
    "                return y_pred[:n].mean().item()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Prediction error on video %s: %s\" % (video_path, str(e)))\n",
    "\n",
    "    finally:\n",
    "        # Clean up temporary faces directory\n",
    "        if os.path.exists(temp_faces_directory):\n",
    "            for file in os.listdir(temp_faces_directory):\n",
    "                file_path = os.path.join(temp_faces_directory, file)\n",
    "                if os.path.isfile(file_path):\n",
    "                    os.remove(file_path)\n",
    "            os.rmdir(temp_faces_directory)\n",
    "\n",
    "    return 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5c66207c-b732-4d4a-92bc-20f33445905e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_on_video_set(videos):\n",
    "    predictions = []\n",
    "    for filename in videos:\n",
    "        y_pred = predict_on_video(os.path.join(test_dir, filename), batch_size=frames_per_video)\n",
    "        predictions.append(y_pred)\n",
    "\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8305c25a-7a34-4599-af92-9c31187bfb49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(len(test_videos[78:79]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "59f7a518-b210-486f-bd7c-c0a8422d7305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction complete. 72 faces extracted.\n",
      "Extraction complete. 100 faces extracted.\n",
      "Extraction complete. 100 faces extracted.\n",
      "Extraction complete. 100 faces extracted.\n",
      "Extraction complete. 100 faces extracted.\n",
      "CPU times: total: 1min 46s\n",
      "Wall time: 2min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.eval()\n",
    "predictions = predict_on_video_set(test_videos[15:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9d4a04ea-31a6-488d-b15d-9fce2cb5b32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df_xception = pd.DataFrame({\"filename\": test_videos[:5], \"label\": predictions})\n",
    "submission_df_xception[\"prediction_label\"] = submission_df_xception[\"label\"].apply(lambda x: \"real\" if x < 0.5 else \"fake\")\n",
    "submission_df_xception.to_csv(\"submission_xception.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4f8abd28-fe61-47f7-9929-8b0ca59b625c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>label</th>\n",
       "      <th>prediction_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.mp4</td>\n",
       "      <td>0.922967</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100.mp4</td>\n",
       "      <td>0.792968</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>102.mp4</td>\n",
       "      <td>0.989279</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1023.mp4</td>\n",
       "      <td>0.965471</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1025.mp4</td>\n",
       "      <td>0.950552</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   filename     label prediction_label\n",
       "0     1.mp4  0.922967             fake\n",
       "1   100.mp4  0.792968             fake\n",
       "2   102.mp4  0.989279             fake\n",
       "3  1023.mp4  0.965471             fake\n",
       "4  1025.mp4  0.950552             fake"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df_xception.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b5139db9-db0d-4b2f-b619-fee0207e19a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model: 0.0\n"
     ]
    }
   ],
   "source": [
    "ground_truth = [\"real\"] * len(test_videos[:5])\n",
    "correct_predictions = (submission_df_xception[\"prediction_label\"] == ground_truth).sum()\n",
    "accuracy = correct_predictions / len(test_videos[:5])\n",
    "print(f\"Accuracy of the model: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76866ce7-7ab4-42a4-8df2-1cc087647842",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
